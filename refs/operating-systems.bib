@inproceedings {osdi16-kaestle,
author = {Stefan Kaestle and Reto Achermann and Roni Haecki and Moritz Hoffmann and Sabela Ramos and Timothy Roscoe},
title = {{Machine-Aware} Atomic Broadcast Trees for Multicores},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {Savannah, GA},
pages = {33--48},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/kaestle},
publisher = {USENIX Association},
month = nov
}

@inproceedings {osdi16-litton,
author = {James Litton and Anjo Vahldiek-Oberwagner and Eslam Elnikety and Deepak Garg and Bobby Bhattacharjee and Peter Druschel},
title = {{Light-Weight} Contexts: An {OS} Abstraction for Safety and Performance},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {Savannah, GA},
pages = {49--64},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/litton},
publisher = {USENIX Association},
month = nov
}

@inproceedings {osdi16-schatzberg,
author = {Dan Schatzberg and James Cadden and Han Dong and Orran Krieger and Jonathan Appavoo},
title = {{EbbRT}: A Framework for Building {Per-Application} Library Operating Systems},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {Savannah, GA},
pages = {671--688},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/schatzberg},
publisher = {USENIX Association},
month = nov
}

@inproceedings {osdi16-kwon,
author = {Youngjin Kwon and Hangchen Yu and Simon Peter and Christopher J. Rossbach and Emmett Witchel},
title = {Coordinated and Efficient Huge Page Management with Ingens},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {Savannah, GA},
pages = {705--721},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/kwon},
publisher = {USENIX Association},
month = nov
}

@inproceedings{eurosys16-dong,
author = {Dong, Xiaowan and Dwarkadas, Sandhya and Cox, Alan L.},
title = {Shared address translation revisited},
year = {2016},
isbn = {9781450342407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901318.2901327},
doi = {10.1145/2901318.2901327},
abstract = {Modern operating systems avoid duplication of code and data when they are mapped by multiple processes by sharing physical memory through mechanisms like copy-on-write. Nonetheless, a separate copy of the virtual address translation structures, such as page tables, are still maintained for each process, even if they are identical. This duplication can lead to inefficiencies in the address translation process and interference within the memory hierarchy. In this paper, we show that on Android platforms, sharing address translation structures, specifically, page tables and TLB entries, for shared libraries can improve performance. For example, at a low level, sharing address translation structures reduces the cost of fork by more than half by reducing page table construction overheads. At a higher level, application launch and IPC are faster due to page fault elimination coupled with better cache and TLB performance when context switching.},
booktitle = {Proceedings of the Eleventh European Conference on Computer Systems},
articleno = {18},
numpages = {15},
location = {London, United Kingdom},
series = {EuroSys '16}
}

@inproceedings {atc16-kashyap,
author = {Sanidhya Kashyap and Changwoo Min and Byoungyoung Lee and Taesoo Kim and Pavel Emelyanov},
title = {Instant {OS} Updates via Userspace {Checkpoint-and-Restart}},
booktitle = {2016 USENIX Annual Technical Conference (USENIX ATC 16)},
year = {2016},
isbn = {978-1-931971-30-0},
address = {Denver, CO},
pages = {605--619},
url = {https://www.usenix.org/conference/atc16/technical-sessions/presentation/kashyap},
publisher = {USENIX Association},
month = jun
}

@inproceedings{asplos16-hajj,
author = {El Hajj, Izzat and Merritt, Alexander and Zellweger, Gerd and Milojicic, Dejan and Achermann, Reto and Faraboschi, Paolo and Hwu, Wen-mei and Roscoe, Timothy and Schwan, Karsten},
title = {SpaceJMP: Programming with Multiple Virtual Address Spaces},
year = {2016},
isbn = {9781450340915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2872362.2872366},
doi = {10.1145/2872362.2872366},
abstract = {Memory-centric computing demands careful organization of the virtual address space, but traditional methods for doing so are inflexible and inefficient. If an application wishes to address larger physical memory than virtual address bits allow, if it wishes to maintain pointer-based data structures beyond process lifetimes, or if it wishes to share large amounts of memory across simultaneously executing processes, legacy interfaces for managing the address space are cumbersome and often incur excessive overheads. We propose a new operating system design that promotes virtual address spaces to first-class citizens, enabling process threads to attach to, detach from, and switch between multiple virtual address spaces. Our work enables data-centric applications to utilize vast physical memory beyond the virtual range, represent persistent pointer-rich data structures without special pointer representations, and share large amounts of memory between processes efficiently.We describe our prototype implementations in the DragonFly BSD and Barrelfish operating systems. We also present programming semantics and a compiler transformation to detect unsafe pointer usage. We demonstrate the benefits of our work on data-intensive applications such as the GUPS benchmark, the SAMTools genomics workflow, and the Redis key-value store.},
booktitle = {Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {353–368},
numpages = {16},
keywords = {virtual memory, memory-centric computing, address spaces, NVM, DRAM},
location = {Atlanta, Georgia, USA},
series = {ASPLOS '16}
}

@inproceedings{sosp17-roghanchi,
author = {Roghanchi, Sepideh and Eriksson, Jakob and Basu, Nilanjana},
title = {ffwd: delegation is (much) faster than you think},
year = {2017},
isbn = {9781450350853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132747.3132771},
doi = {10.1145/3132747.3132771},
abstract = {We revisit the question of delegation vs. synchronized access to shared memory, and show through analysis and demonstration that delegation can be much faster than locking under a range of common circumstances. Starting from first principles, we propose fast, fly-weight delegation (ffwd). The highly optimized design of ffwd allows it to significantly outperform prior work on delegation, while retaining the scalability advantage.In experiments with 6 benchmark applications, and 6 shared data structures, running on four different multi-socket systems with up to 128 hardware threads, we compare ffwd to a selection of lock, combining, lock-free, software transactional memory and delegation designs. Overall, we find that ffwd often offers a simple and highly competitive alternative to existing work. By definition, the performance of a fully delegated data structure is limited by the single-thread throughput of said data structure. However, due to cache effects, many data structures offer their best performance when confined to a single thread. With an efficient delegation mechanism, we approach this single-threaded performance in a multi-threaded setting. In application-level benchmarks, we see improvements up to 100\% over the next best solution tested (RCL), and multiple micro-benchmarks show improvements in the 5-10x range.},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
pages = {342–358},
numpages = {17},
location = {Shanghai, China},
series = {SOSP '17}
}

@inproceedings{eurosys17-teabe,
author = {Teabe, Boris and Nitu, Vlad and Tchana, Alain and Hagimont, Daniel},
title = {The lock holder and the lock waiter pre-emption problems: nip them in the bud using informed spinlocks (I-Spinlock)},
year = {2017},
isbn = {9781450349383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3064176.3064180},
doi = {10.1145/3064176.3064180},
abstract = {In native Linux systems, spinlock's implementation relies on the assumption that both the lock holder thread and lock waiter threads cannot be preempted. However, in a virtualized environment, these threads are scheduled on top of virtual CPUs (vCPU) that can be preempted by the hypervisor at any time, thus forcing lock waiter threads on other vCPUs to busy wait and to waste CPU cycles. This leads to the well-known Lock Holder Preemption (LHP) and Lock Waiter Preemption (LWP) issues.In this paper, we propose I-Spinlock (for Informed Spinlock), a new spinlock implementation for virtualized environments. Its main principle is to only allow a thread to acquire a lock if and only if the remaining time-slice of its vCPU is sufficient to enter and leave the critical section. This is possible if the spinlock primitive is aware (informed) of its time-to-preemption (by the hypervisor).We implemented I-Spinlock in the Xen virtualization system. We show that our solution is compliant with both para-virtual and hardware virtualization modes. We performed extensive performance evaluations with various reference benchmarks and compared our solution to previous solutions. The evaluations demonstrate that I-Spinlock outperforms other solutions, and more significantly when the number of core increases.},
booktitle = {Proceedings of the Twelfth European Conference on Computer Systems},
pages = {286–297},
numpages = {12},
keywords = {multi-core, scheduler, spinlocks, virtual machine},
location = {Belgrade, Serbia},
series = {EuroSys '17}
}

@inproceedings {atc17-amit,
author = {Nadav Amit},
title = {Optimizing the {TLB} Shootdown Algorithm with Page Access Tracking},
booktitle = {2017 USENIX Annual Technical Conference (USENIX ATC 17)},
year = {2017},
isbn = {978-1-931971-38-6},
address = {Santa Clara, CA},
pages = {27--39},
url = {https://www.usenix.org/conference/atc17/technical-sessions/presentation/amit},
publisher = {USENIX Association},
month = jul
}

@inproceedings {222601,
author = {Yizhou Shan and Yutong Huang and Yilun Chen and Yiying Zhang},
title = {{LegoOS}: A Disseminated, Distributed {OS} for Hardware Resource Disaggregation},
booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
year = {2018},
isbn = {978-1-939133-08-3},
address = {Carlsbad, CA},
pages = {69--87},
url = {https://www.usenix.org/conference/osdi18/presentation/shan},
publisher = {USENIX Association},
month = oct,
code = {https://github.com/WukLab/LegoOS},
}

@inproceedings {222627,
author = {Cody Cutler and M. Frans Kaashoek and Robert T. Morris},
title = {The benefits and costs of writing a {POSIX} kernel in a high-level language},
booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
year = {2018},
isbn = {978-1-939133-08-3},
address = {Carlsbad, CA},
pages = {89--105},
url = {https://www.usenix.org/conference/osdi18/presentation/cutler},
publisher = {USENIX Association},
month = oct
}

@inproceedings{eurosys18-lyons,
author = {Lyons, Anna and McLeod, Kent and Almatary, Hesham and Heiser, Gernot},
title = {Scheduling-context capabilities: a principled, light-weight operating-system mechanism for managing time},
year = {2018},
isbn = {9781450355841},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3190508.3190539},
doi = {10.1145/3190508.3190539},
abstract = {Mixed-criticality systems (MCS) combine real-time components of different levels of criticality - i.e. severity of failure - on the same processor, in order to obtain good resource utilisation. They must be able to guarantee deadlines of highly-critical threads without any dependence on less-critical threads. This requires strong temporal isolation, similar to the spatial isolation that is traditionally provided by operating systems, without unnecessary loss of processor utilisation. We present a model that uses scheduling contexts as first-class objects to represent time, and integrates seamlessly with the capability-based protection model of the seL4 microkernel. We show that the model comes with minimal overhead, and supports implementation of arbitrary scheduling policies as well as criticality switches at user level.},
booktitle = {Proceedings of the Thirteenth EuroSys Conference},
articleno = {26},
numpages = {16},
keywords = {access control, capabilities, microkernels, mixed-criticality systems, seL4},
location = {Porto, Portugal},
series = {EuroSys '18}
}

@inproceedings{eurosys18-zhao,
author = {Zhao, Ming and Cabrera, Jorge},
title = {RTVirt: enabling time-sensitive computing on virtualized systems through cross-layer CPU scheduling},
year = {2018},
isbn = {9781450355841},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3190508.3190527},
doi = {10.1145/3190508.3190527},
abstract = {Virtualization enables flexible application delivery and efficient resource consolidation, and is pervasively used to build various virtualized systems including public and private cloud computing systems. Many applications can benefit from computing on virtualized systems, including those that are time sensitive, but it is still challenging for existing virtualized systems to deliver application-desired timeliness. In particular, the lack of awareness between VM host- and guest-level schedulers presents a serious hurdle to achieving strong timeliness guarantees on virtualized systems. This paper presents RTVirt, a new solution to time-sensitive computing on virtualized systems through cross-layer scheduling. It allows the two levels of schedulers on a virtualized system to communicate key scheduling information and coordinate on the scheduling decisions. It enables optimal multiprocessor schedulers to support virtualized time-sensitive applications with strong timeliness guarantees and efficient resource utilization. RTVirt is prototyped on a widely used virtualization framework (Xen) and evaluated with diverse workloads. The results show that it can meet application deadlines (99\%) or tail latency requirements (99.9th percentile) nearly perfectly; it can handle large numbers of applications and dynamic changes in their timeliness requirements; and it substantially outperforms the existing solutions in both timeliness and resource utilization.},
booktitle = {Proceedings of the Thirteenth EuroSys Conference},
articleno = {27},
numpages = {13},
keywords = {virtualization, time-sensitive computing, cloud computing},
location = {Porto, Portugal},
series = {EuroSys '18}
}

@inproceedings{eurosys18-ahn,
author = {Ahn, Jeongseob and Park, Chang Hyun and Heo, Taekyung and Huh, Jaehyuk},
title = {Accelerating critical OS services in virtualized systems with flexible micro-sliced cores},
year = {2018},
isbn = {9781450355841},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3190508.3190521},
doi = {10.1145/3190508.3190521},
abstract = {Consolidating multiple virtual machines into a single server has been widely adopted in cloud computing to improve system utilization. However, the sharing of physical CPUs among virtual machines in consolidated systems poses a new challenge in providing an illusion of continuous CPU execution to the guest operating systems (OS). Due to the time-sharing of physical CPUs, the execution of a guest OS is periodically interrupted, while the guest OS may not be aware of the discontinuity of virtual time against the real time. The virtual time discontinuity problem causes the delayed processing of critical OS operations, such as interrupt handling and lock processing. Although there have been several prior studies to mitigate the problem, they address only a subset of symptoms, require the modification of guest OSes, or change the processor architecture. This paper proposes a novel way to comprehensively reduce the inefficiency of guest OS execution in consolidated systems. It migrates the short-lived critical OS tasks to dedicated micro-sliced cores, minimizing the delays caused by time sharing. The hypervisor identifies the critical OS tasks without any OS intervention, and schedules the critical code sections onto the dynamically partitioned cores at runtime. The dedicated micro-sliced cores employ a short sub-millisecond quantum to minimize the response latencies for consolidated virtual machines. By readily servicing the critical tasks, the proposed scheme can minimize the adverse artifact of virtual machine consolidation without any modification of guest OSes.},
booktitle = {Proceedings of the Thirteenth EuroSys Conference},
articleno = {29},
numpages = {14},
keywords = {virtualization, virtual time discontinuity, scheduling},
location = {Porto, Portugal},
series = {EuroSys '18}
}

@inproceedings{eurosys18-kashyap,
author = {Kashyap, Sanidhya and Min, Changwoo and Kim, Kangnyeon and Kim, Taesoo},
title = {A scalable ordering primitive for multicore machines},
year = {2018},
isbn = {9781450355841},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3190508.3190510},
doi = {10.1145/3190508.3190510},
abstract = {Timestamping is an essential building block for designing concurrency control mechanisms and concurrent data structures. Various algorithms either employ physical timestamping, assuming that they have access to synchronized clocks, or maintain a logical clock with the help of atomic instructions. Unfortunately, these approaches have two problems. First, hardware developers do not guarantee that the available hardware clocks are exactly synchronized, which they find difficult to achieve in practice. Second, the atomic instructions are a deterrent to scalability resulting from cache-line contention. This paper addresses these problems by proposing and designing a scalable ordering primitive, called Ordo, that relies on invariant hardware clocks. Ordo not only enables the correct use of these clocks, by providing a notion of a global hardware clock, but also frees various logical timestamp-based algorithms from the burden of the software logical clock, while trying to simplify their design. We use the Ordo primitive to redesign 1) a concurrent data structure library that we apply on the Linux kernel; 2) a synchronization mechanism for concurrent programming; 3) two database concurrency control mechanisms; and 4) a clock-based software transactional memory algorithm. Our evaluation shows that there is a possibility that the clocks are not synchronized on two architectures (Intel and ARM) and that Ordo generally improves the efficiency of several algorithms by 1.2--39.7X on various architectures.},
booktitle = {Proceedings of the Thirteenth EuroSys Conference},
articleno = {34},
numpages = {15},
location = {Porto, Portugal},
series = {EuroSys '18}
}

@inproceedings {atc18-zhang,
author = {Yiming Zhang and Jon Crowcroft and Dongsheng Li and Chengfen Zhang and Huiba Li and Yaozheng Wang and Kai Yu and Yongqiang Xiong and Guihai Chen},
title = {{KylinX}: A Dynamic Library Operating System for Simplified and Efficient Cloud Virtualization},
booktitle = {2018 USENIX Annual Technical Conference (USENIX ATC 18)},
year = {2018},
isbn = {ISBN 978-1-939133-01-4},
address = {Boston, MA},
pages = {173--186},
url = {https://www.usenix.org/conference/atc18/presentation/zhang-yiming},
publisher = {USENIX Association},
month = jul
}

@inproceedings{sosp19-marty,
author = {Marty, Michael and de Kruijf, Marc and Adriaens, Jacob and Alfeld, Christopher and Bauer, Sean and Contavalli, Carlo and Dalton, Michael and Dukkipati, Nandita and Evans, William C. and Gribble, Steve and Kidd, Nicholas and Kononov, Roman and Kumar, Gautam and Mauer, Carl and Musick, Emily and Olson, Lena and Rubow, Erik and Ryan, Michael and Springborn, Kevin and Turner, Paul and Valancius, Valas and Wang, Xi and Vahdat, Amin},
title = {Snap: a microkernel approach to host networking},
year = {2019},
isbn = {9781450368735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341301.3359657},
doi = {10.1145/3341301.3359657},
abstract = {This paper presents our design and experience with a microkernel-inspired approach to host networking called Snap. Snap is a userspace networking system that supports Google's rapidly evolving needs with flexible modules that implement a range of network functions, including edge packet switching, virtualization for our cloud platform, traffic shaping policy enforcement, and a high-performance reliable messaging and RDMA-like service. Snap has been running in production for over three years, supporting the extensible communication needs of several large and critical systems.Snap enables fast development and deployment of new networking features, leveraging the benefits of address space isolation and the productivity of userspace software development together with support for transparently upgrading networking services without migrating applications off of a machine. At the same time, Snap achieves compelling performance through a modular architecture that promotes principled synchronization with minimal state sharing, and supports real-time scheduling with dynamic scaling of CPU resources through a novel kernel/userspace CPU scheduler co-design. Our evaluation demonstrates over 3x Gbps/core improvement compared to a kernel networking stack for RPC workloads, software-based RDMA-like performance of up to 5M IOPS/core, and transparent upgrades that are largely imperceptible to user applications. Snap is deployed to over half of our fleet of machines and supports the needs of numerous teams.},
booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
pages = {399–413},
numpages = {15},
keywords = {network stack, microkernel, datacenter, RDMA},
location = {Huntsville, Ontario, Canada},
series = {SOSP '19}
}

@inproceedings{eurosys19-mi,
author = {Mi, Zeyu and Li, Dingji and Yang, Zihan and Wang, Xinran and Chen, Haibo},
title = {SkyBridge: Fast and Secure Inter-Process Communication for Microkernels},
year = {2019},
isbn = {9781450362818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302424.3303946},
doi = {10.1145/3302424.3303946},
abstract = {Microkernels have been extensively studied over decades. However, IPC (Inter-Process Communication) is still a major factor of runtime overhead, where fine-grained isolation usually leads to excessive IPCs. The main overhead of IPC comes from the involvement of the kernel, which includes the direct cost of mode switches and address space changes, as well as indirect cost due to the pollution of processor structures.In this paper, we present SkyBridge, a new communication facility designed and optimized for synchronous IPC in microkernels. SkyBridge requires no involvement of kernels during communication and allows a process to directly switch to the virtual address space of the target process and invoke the target function. SkyBridge retains the traditional virtual address space isolation and thus can be easily integrated into existing microkernels. The key idea of SkyBridge is to leverage a commodity hardware feature for virtualization (i.e., VMFUNC) to achieve efficient IPC. To leverage the hardware feature, SkyBridge inserts a tiny virtualization layer (Rootkernel) beneath the original microkernel (Subkernel). The Rootkernel is carefully designed to eliminate most virtualization overheads. SkyBridge also integrates a series of techniques to guarantee the security properties of IPC.We have implemented SkyBridge on three popular open-source microkernels (seL4, Fiasco.OC, and Google Zircon). The evaluation results show that SkyBridge improves the speed of IPC by 1.49x to 19.6x for microbenchmarks. For real-world applications (e.g., SQLite3 database), SkyBridge improves the throughput by 81.9\%, 1.44x and 9.59x for the three microkernels on average.},
booktitle = {Proceedings of the Fourteenth EuroSys Conference 2019},
articleno = {9},
numpages = {15},
location = {Dresden, Germany},
series = {EuroSys '19}
}

@inproceedings{eurosys19-lochmann,
author = {Lochmann, Alexander and Schirmeier, Horst and Borghorst, Hendrik and Spinczyk, Olaf},
title = {LockDoc: Trace-Based Analysis of Locking in the Linux Kernel},
year = {2019},
isbn = {9781450362818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302424.3303948},
doi = {10.1145/3302424.3303948},
abstract = {For fine-grained synchronization of application and kernel threads, the Linux kernel provides a multitude of different locking mechanisms that are being used on various individually locked data structures. Understanding which locks are required in which order for a particular member variable of a kernel data structure has become truly difficult, even for Linux-kernel experts themselves.In this paper we introduce LockDoc -- an approach that, based on the analysis of execution traces of an instrumented Linux kernel, automatically deduces the most likely locking rule for all members of arbitrary kernel data structures. From these locking rules, LockDoc generates documentation that supports kernel developers and helps avoiding concurrency bugs. Additionally, the (very limited) existing documentation can be verified, and locking-rule violations -- potential bugs in the kernel code -- can be found.Our results include generated locking rules for previously predominantly undocumented member variables of 11 different Linux-kernel data structures. Manually inspecting the scarce source-code documentation for five of these data structures reveals that only 53 percent of the variables with a documented locking rule are actually consistently accessed with the required locks held. This indicates possible documentation or synchronization bugs in the Linux kernel, of which one has already been confirmed by kernel experts.},
booktitle = {Proceedings of the Fourteenth EuroSys Conference 2019},
articleno = {11},
numpages = {15},
location = {Dresden, Germany},
series = {EuroSys '19}
}

@inproceedings{asplos19-panwar,
author = {Panwar, Ashish and Bansal, Sorav and Gopinath, K.},
title = {HawkEye: Efficient Fine-grained OS Support for Huge Pages},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304064},
doi = {10.1145/3297858.3304064},
abstract = {Effective huge page management in operating systems is necessary for mitigation of address translation overheads. However, this continues to remain a difficult area in OS design. Recent work on Ingens uncovered some interesting pitfalls in current huge page management strategies. Using both page access patterns discovered by the OS kernel and fine-grained data from hardware performance counters, we expose problematic aspects of current huge page management strategies. In our system, called HawkEye/Linux, we demonstrate alternate ways to address issues related to performance, page fault latency and memory bloat; the primary ideas behind HawkEye management algorithms are async page pre-zeroing, de-duplication of zero-filled pages, fine-grained page access tracking and measurement of address translation overheads through hardware performance counters. Our evaluation shows that HawkEye is more performant, robust and better-suited to handle diverse workloads when compared with current state-of-the-art systems.},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {347–360},
numpages = {14},
keywords = {hardware counters, huge pages, virtual memory},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@inproceedings {osdi20-boos,
author = {Kevin Boos and Namitha Liyanage and Ramla Ijaz and Lin Zhong},
title = {Theseus: an Experiment in Operating System Structure and State Management},
booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {1--19},
url = {https://www.usenix.org/conference/osdi20/presentation/boos},
publisher = {USENIX Association},
month = nov
}

@inproceedings {osdi20-narayanan,
author = {Vikram Narayanan and Tianjiao Huang and David Detweiler and Dan Appel and Zhaofeng Li and Gerd Zellweger and Anton Burtsev},
title = {{RedLeaf}: Isolation and Communication in a Safe Operating System},
booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {21--39},
url = {https://www.usenix.org/conference/osdi20/presentation/narayanan-vikram},
publisher = {USENIX Association},
month = nov
}

@inproceedings{eurosys20-nordholz,
author = {Nordholz, Jan},
title = {Design of a symbolically executable embedded hypervisor},
year = {2020},
isbn = {9781450368827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342195.3387516},
doi = {10.1145/3342195.3387516},
abstract = {Hypervisor implementations such as XMHF, Nova, PROSPER, prplHypervisor, the various L4 descendants, as well as KVM and Xen offer mechanisms for dynamic startup and reconfiguration, including the allocation, delegation and destruction of objects and resources at runtime. Some use cases such as cloud computing depend on this dynamicity, yet its inclusion also renders the state space intractable to simulation-based verification tools. On the other hand, system architectures for embedded devices are often fixed in the number and properties of isolated tasks, therefore a much simpler, less dynamic hypervisor design would suffice. We close this design gap by presenting Phidias, a new hypervisor consisting of a minimal runtime codebase that is almost devoid of dynamicity, and a comprehensive compile-time configuration framework. We then leverage this lack of dynamic components to non-interactively verify the validity of certain invariants. Specifically, we verify hypervisor integrity by subjecting the compiled hypervisor binary to our own symbolic execution engine. Finally, we discuss our results, point out possible improvements, and hint at unexplored characteristics of a static hypervisor design.},
booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
articleno = {6},
numpages = {16},
keywords = {symbolic execution, hypervisor, embedded, SAT solving},
location = {Heraklion, Greece},
series = {EuroSys '20}
}

@inproceedings{eurosys20-kogan,
author = {Kogan, Alex and Dice, Dave and Issa, Shady},
title = {Scalable range locks for scalable address spaces and beyond},
year = {2020},
isbn = {9781450368827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342195.3387533},
doi = {10.1145/3342195.3387533},
abstract = {Range locks are a synchronization construct designed to provide concurrent access to multiple threads (or processes) to disjoint parts of a shared resource. Originally conceived in the file system context, range locks are gaining increasing interest in the Linux kernel community seeking to alleviate bottlenecks in the virtual memory management subsystem. The existing implementation of range locks in the kernel, however, uses an internal spin lock to protect the underlying tree structure that keeps track of acquired and requested ranges. This spin lock becomes a point of contention on its own when the range lock is frequently acquired. Furthermore, where and exactly how specific (refined) ranges can be locked remains an open question.In this paper, we make two independent, but related contributions. First, we propose an alternative approach for building range locks based on linked lists. The lists are easy to maintain in a lock-less fashion, and in fact, our range locks do not use any internal locks in the common case. Second, we show how the range of the lock can be refined in the mprotect operation through a speculative mechanism. This refinement, in turn, allows concurrent execution of mprotect operations on non-overlapping memory regions. We implement our new algorithms and demonstrate their effectiveness in user-space and kernel-space, achieving up to 9X speedup compared to the stock version of the Linux kernel. Beyond the virtual memory management subsystem, we discuss other applications of range locks in parallel software. As a concrete example, we show how range locks can be used to facilitate the design of scalable concurrent data structures, such as skip lists.},
booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
articleno = {8},
numpages = {15},
keywords = {linux kernel, lock-less, parallel file systems, reader-writer locks, scalable synchronization, semaphores},
location = {Heraklion, Greece},
series = {EuroSys '20}
}

@inproceedings{eurosys20-kuo,
author = {Kuo, Hsuan-Chi and Williams, Dan and Koller, Ricardo and Mohan, Sibin},
title = {A Linux in unikernel clothing},
year = {2020},
isbn = {9781450368827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342195.3387526},
doi = {10.1145/3342195.3387526},
abstract = {Unikernels leverage library OS architectures to run isolated workloads on the cloud. They have garnered attention in part due to their promised performance characteristics such as small image size, fast boot time, low memory footprint and application performance. However, those that aimed at generality fall short of the application compatibility, robustness and, more importantly, community that is available for Linux. In this paper, we describe and evaluate Lupine Linux, a standard Linux system that---through kernel configuration specialization and system call overhead elimination---achieves unikernel-like performance, in fact outperforming at least one reference unikernel in all of the above dimensions. At the same time, Lupine can run any application (since it is Linux) when faced with more general workloads, whereas many unikernels simply crash. We demonstrate a graceful degradation of unikernel-like performance properties.},
booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
articleno = {11},
numpages = {15},
location = {Heraklion, Greece},
series = {EuroSys '20}
}

@inproceedings{eurosys20-amit,
author = {Amit, Nadav and Tai, Amy and Wei, Michael},
title = {Don't shoot down TLB shootdowns!},
year = {2020},
isbn = {9781450368827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342195.3387518},
doi = {10.1145/3342195.3387518},
abstract = {Translation Lookaside Buffers (TLBs) are critical for building performant virtual memory systems. Because most processors do not provide coherence for TLB mappings, TLB shootdowns provide a software mechanism that invokes inter-processor interrupts (IPLs) to synchronize TLBs. TLB shootdowns are expensive, so recent work has aimed to avoid the frequency of shootdowns through techniques such as batching. We show that aggressive batching can cause correctness issues and addressing them can obviate the benefits of batching. Instead, our work takes a different approach which focuses on both improving the performance of TLB shootdowns and carefully selecting where to avoid shootdowns. We introduce four general techniques to improve shootdown performance: (1) concurrently flush initiator and remote TLBs, (2) early acknowledgement from remote cores, (3) cacheline consolidation of kernel data structures to reduce cacheline contention, and (4) in-context flushing of userspace entries to address the overheads introduced by Spectre and Meltdown mitigations. We also identify that TLB flushing can be avoiding when handling copy-on-write (CoW) faults and some TLB shootdowns can be batched in certain system calls. Overall, we show that our approach results in significant speedups without sacrificing safety and correctness in both microbenchmarks and real-world applications.},
booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
articleno = {35},
numpages = {14},
location = {Heraklion, Greece},
series = {EuroSys '20}
}

@inproceedings {atc20-gu,
author = {Jinyu Gu and Xinyue Wu and Wentai Li and Nian Liu and Zeyu Mi and Yubin Xia and Haibo Chen},
title = {Harmonizing Performance and Isolation in Microkernels with Efficient Intra-kernel Isolation and Communication},
booktitle = {2020 USENIX Annual Technical Conference (USENIX ATC 20)},
year = {2020},
isbn = {978-1-939133-14-4},
pages = {401--417},
url = {https://www.usenix.org/conference/atc20/presentation/gu},
publisher = {USENIX Association},
month = jul
}

@inproceedings {osdi21-ibanez,
author = {Stephen Ibanez and Alex Mallery and Serhat Arslan and Theo Jepsen and Muhammad Shahbaz and Changhoon Kim and Nick McKeown},
title = {The nanoPU: A Nanosecond Network Stack for Datacenters},
booktitle = {15th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 21)},
year = {2021},
isbn = {978-1-939133-22-9},
pages = {239--256},
url = {https://www.usenix.org/conference/osdi21/presentation/ibanez},
publisher = {{USENIX} Association},
month = jul
}

@inproceedings {osdi21-bhardwaj,
author = {Ankit Bhardwaj and Chinmay Kulkarni and Reto Achermann and Irina Calciu and Sanidhya Kashyap and Ryan Stutsman and Amy Tai and Gerd Zellweger},
title = {NrOS: Effective Replication and Sharing in an Operating System},
booktitle = {15th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 21)},
year = {2021},
isbn = {978-1-939133-22-9},
pages = {295--312},
url = {https://www.usenix.org/conference/osdi21/presentation/bhardwaj},
publisher = {{USENIX} Association},
month = jul
}

@inproceedings{sosp21-li,
author = {Li, Dingji and Mi, Zeyu and Xia, Yubin and Zang, Binyu and Chen, Haibo and Guan, Haibing},
title = {TwinVisor: Hardware-isolated Confidential Virtual Machines for ARM},
year = {2021},
isbn = {9781450387095},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477132.3483554},
doi = {10.1145/3477132.3483554},
abstract = {Confidential VM, which offers an isolated execution environment for cloud tenants with limited trust in the cloud provider, has recently been deployed in major clouds such as AWS and Azure. However, while ARM has become increasingly popular in cloud data centers, existing confidential VM designs mainly leverage specialized x86 hardware extensions (e.g., AMD SEV and Intel TDX) to isolate VMs upon a shared hypervisor.This paper proposes TwinVisor, the first system that enables the hardware-enforced isolation of confidential VMs on ARM platforms. TwinVisor takes advantage of the mature ARM TrustZone to run two isolated hypervisors, one in the secure world (called S-visor in this paper) and the other in the normal world (called N-visor), to support normal VMs and confidential VMs respectively. Instead of building a new S-visor from scratch, our design decouples protection from resource management, and reuses most functionalities of a full-fledged N-visor to minimize the size of S-visor. We have built two prototypes of TwinVisor: one on an official ARM simulator with S-EL2 enabled to validate functional correctness and the other on an ARM development board to evaluate performance. The S-visor comprises 5.8K LoCs while the N-visor introduces 906 LoC changes to KVM. According to our evaluation, TwinVisor can run unmodified VM images as confidential VMs while incurring less than 5\% performance overhead for various real-world workloads on SMP VMs.},
booktitle = {Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles},
pages = {638–654},
numpages = {17},
keywords = {Virtualization, Confidential Computing, Cloud Computing, ARM TrustZone},
location = {Virtual Event, Germany},
series = {SOSP '21}
}

@inproceedings{eurosys21-russinovich,
author = {Russinovich, Mark and Govindaraju, Naga and Raghuraman, Melur and Hepkin, David and Schwartz, Jamie and Kishan, Arun},
title = {Virtual machine preserving host updates for zero day patching in public cloud},
year = {2021},
isbn = {9781450383349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447786.3456232},
doi = {10.1145/3447786.3456232},
abstract = {Host software updates are critical to ensure the security, reliability and compliance of public clouds. Many updates require a virtualization component restart or operating system reboot. Virtual machines (VMs) running on the updated servers must either be restarted or live migrated off. Reboots can result in downtime for the VMs on the order of ten minutes, and has further impact on the workloads running in the VMs because cached state is lost. Live migration (LM) is a technology that can avoid the need to shutdown VMs. However, LM requires turn space in the form of already-patched hosts, consumes network, CPU and other resources that scale with the amount of and level of activity of VM, and has variable impact on VM performance and availability, making it too expensive and disruptive for zero-day security updates that must be applied across an entire fleet on the order of hours. We present a novel update technology, virtual machine preserving host updates (VM-PHU), that does not require turn space, consumes no network and little CPU, preserves VM state, and causes minimal VM blackout time that does not scale with VM resource usage. VM-PHU persists the memory and device state of all running guest VMs, reboots the host and virtualization components into updated code, restores the state of the VMs, and then resumes them. VM-PHU makes use of several techniques to minimize VM blackout time. One is to use kernel soft reboot (KSR) to directly transition to an updated host operating system, bypassing firmware reset of the server and attached devices. To minimize resource consumption and VM disruption, VM-PHU leaves VM memory in physical memory pages and other state in persisted pages across the soft reboot, and VM-PHU implements a mechanism called fast close to enable a reboot to proceed without waiting for the completion of in-flight VM I/Os to remote storage devices. We have implemented VM-PHU in Microsoft Azure hosting millions of servers and show results of several zero-day updates that demonstrate VM blackout times on the order of seconds. VM-PHU provides significant benefits to both customers and public cloud vendors by minimizing application downtime while enabling fast and resource efficient updates, including zero-day patches.},
booktitle = {Proceedings of the Sixteenth European Conference on Computer Systems},
pages = {114–129},
numpages = {16},
keywords = {cloud computing, data centers, virtualization and security},
location = {Online Event, United Kingdom},
series = {EuroSys '21}
}

@inproceedings{eurosys21-ngoc,
author = {Ngoc, Tu Dinh and Teabe, Boris and Tchana, Alain and Muller, Gilles and Hagimont, Daniel},
title = {Mitigating vulnerability windows with hypervisor transplant},
year = {2021},
isbn = {9781450383349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447786.3456235},
doi = {10.1145/3447786.3456235},
abstract = {The vulnerability window of a hypervisor regarding a given security flaw is the time between the identification of the flaw and the integration of a correction/patch in the running hypervisor. Most vulnerability windows, regardless of severity, are long enough (several days) that attackers have time to perform exploits. Nevertheless, the number of critical vulnerabilities per year is low enough to allow an exceptional solution. This paper introduces hypervisor transplant, a solution for addressing vulnerability window of critical flaws. It involves temporarily replacing the current datacenter hypervisor (e.g., Xen) which is subject to a critical security flaw, by a different hypervisor (e.g., KVM) which is not subject to the same vulnerability.We build HyperTP, a generic framework which combines in a unified way two approaches: in-place server micro-reboot-based hypervisor transplant (noted InPlaceTP) and live VM migration-based hypervisor transplant (noted MigrationTP). We describe the implementation of HyperTP and its extension for transplanting Xen with KVM and vice versa. We also show that HyperTP is easy to integrate with the OpenStack cloud computing platform. Our evaluation results show that HyperTP delivers satisfactory performance: (1) MigrationTP takes the same time and impacts virtual machines (VMs) with the same performance degradation as normal live migration. (2) the downtime imposed by InPlaceTP on VMs is in the same order of magnitude (1.7 seconds for a VM with 1 vCPU and 1 GB of RAM) as in-place upgrade of homogeneous hypervisors based on server micro-reboot.},
booktitle = {Proceedings of the Sixteenth European Conference on Computer Systems},
pages = {162–177},
numpages = {16},
location = {Online Event, United Kingdom},
series = {EuroSys '21}
}

@inproceedings{eurosys21-papagiannis,
author = {Papagiannis, Anastasios and Marazakis, Manolis and Bilas, Angelos},
title = {Memory-mapped I/O on steroids},
year = {2021},
isbn = {9781450383349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447786.3456242},
doi = {10.1145/3447786.3456242},
abstract = {With current technology trends for fast storage devices, the host-level I/O path is emerging as a main bottleneck for modern, data-intensive servers and applications. The need to improve I/O performance requires customizing various aspects of the I/O path, including the page cache and the method to access the storage devices.In this paper, we present Aquila, a library OS that allows applications to reduce I/O overhead by customizing the memory-mapped I/O (mmio) path for files or storage devices. Compared to Linux mmap, Aquila (a) offers full mmio compatibility and protection to minimize application modifications, (b) allows applications to customize the DRAM I/O cache, its policies, and access to storage devices, and (c) significantly reduces I/O overhead. Aquila achieves its mmio compatibility, flexibility, and performance by placing the application in a privileged domain, non-root ring 0.We show the benefits of Aquila in two cases: (a) Using mmio in key-value stores to reduce I/O overhead and (b) utilizing mmio in graph processing applications to extend the memory heap over fast storage devices. Aquila requires 2.58\texttimes{} fewer CPU cycles for cache management in RocksDB, compared to user-space caching and read/write system calls and results in 40\% improvement in request throughput. Finally, we use Ligra, a graph processing framework, to show the efficiency of Aquila in extending the memory heap over fast storage devices. In this case, Aquila results in up to 4.14\texttimes{} lower execution time compared to Linux mmap.},
booktitle = {Proceedings of the Sixteenth European Conference on Computer Systems},
pages = {277–293},
numpages = {17},
keywords = {memory-mapped I/O, key-value stores, fast storage devices, Linux mmap, I/O caching},
location = {Online Event, United Kingdom},
series = {EuroSys '21}
}

@inproceedings{eurosys21-kuenzer,
author = {Kuenzer, Simon and B\u{a}doiu, Vlad-Andrei and Lefeuvre, Hugo and Santhanam, Sharan and Jung, Alexander and Gain, Gaulthier and Soldani, Cyril and Lupu, Costin and Teodorescu, \c{S}tefan and R\u{a}ducanu, Costi and Banu, Cristian and Mathy, Laurent and Deaconescu, R\u{a}zvan and Raiciu, Costin and Huici, Felipe},
title = {Unikraft: fast, specialized unikernels the easy way},
year = {2021},
isbn = {9781450383349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447786.3456248},
doi = {10.1145/3447786.3456248},
abstract = {Unikernels are famous for providing excellent performance in terms of boot times, throughput and memory consumption, to name a few metrics. However, they are infamous for making it hard and extremely time consuming to extract such performance, and for needing significant engineering effort in order to port applications to them. We introduce Unikraft, a novel micro-library OS that (1) fully modularizes OS primitives so that it is easy to customize the unikernel and include only relevant components and (2) exposes a set of composable, performance-oriented APIs in order to make it easy for developers to obtain high performance.Our evaluation using off-the-shelf applications such as nginx, SQLite, and Redis shows that running them on Unikraft results in a 1.7x-2.7x performance improvement compared to Linux guests. In addition, Unikraft images for these apps are around 1MB, require less than 10MB of RAM to run, and boot in around 1ms on top of the VMM time (total boot time 3ms-40ms). Unikraft is a Linux Foundation open source project and can be found at www.unikraft.org.},
booktitle = {Proceedings of the Sixteenth European Conference on Computer Systems},
pages = {376–394},
numpages = {19},
location = {Online Event, United Kingdom},
series = {EuroSys '21}
}

@inproceedings{eurosys21-zhao,
author = {Zhao, Kaiyang and Gong, Sishuai and Fonseca, Pedro},
title = {On-demand-fork: a microsecond fork for memory-intensive and latency-sensitive applications},
year = {2021},
isbn = {9781450383349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447786.3456258},
doi = {10.1145/3447786.3456258},
abstract = {Fork has long been the process creation system call for Unix. At its inception, fork was hailed as an efficient system call due to its use of copy-on-write on memory shared between parent and child processes. However, application memory demand has increased drastically since the early days and the cost incurred by fork to simply set up virtual memory (e.g., copy page tables) is now a concern, even for applications that only require hundreds of MBs of memory. In practice, fork performance already holds back system efficiency and latency across a range of uses cases that fork large processes, such as fault-tolerant systems, serverless frameworks, and testing frameworks.This paper proposes On-demand-fork, a fast implementation of the fork system call specifically designed for applications with large memory footprints. On-demand-fork relies on the observation that copy-on-write can be generalized to page tables, even on commodity hardware. On-demand-fork executes faster than the traditional fork implementation by additionally sharing page tables between parent and child at fork time and selectively copying page tables in small chunks, on-demand, when handling page faults. On-demand-fork is a drop-in replacement for fork that requires no changes to applications or hardware.We evaluated On-demand-fork on a range of micro-benchmarks and real-world workloads. On-demand-fork significantly reduces the fork invocation time and has improved scalability. For processes with 1 GB of allocated memory, On-demand-fork has a 65\texttimes{} performance advantage over Fork. We also evaluated On-demand-fork on testing, fuzzing, and snapshotting workloads of well-known applications, obtaining execution throughput improvements between 59\% and 226\% and up to 99\% invocation latency reduction.},
booktitle = {Proceedings of the Sixteenth European Conference on Computer Systems},
pages = {540–555},
numpages = {16},
location = {Online Event, United Kingdom},
series = {EuroSys '21}
}

@inproceedings{asplos21-sartakov,
author = {Sartakov, Vasily A. and Vilanova, Llu\'{\i}s and Pietzuch, Peter},
title = {CubicleOS: a library OS with software componentisation for practical isolation},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446731},
doi = {10.1145/3445814.3446731},
abstract = {Library OSs have been proposed to deploy applications isolated inside containers, VMs, or trusted execution environments. They often follow a highly modular design in which third-party components are combined to offer the OS functionality needed by an application, and they are customised at compilation and deployment time to fit application requirements. Yet their monolithic design lacks isolation across components: when applications and OS components contain security-sensitive data (e.g., cryptographic keys or user data), the lack of isolation renders library OSs open to security breaches via malicious or vulnerable third-party components. We describe CubicleOS, a library OS that isolates components in the system while maintaining the simple, monolithic development approach of library composition. CubicleOS allows isolated components, called cubicles , to share data dynamically with other components. It provides spatial memory isolation at the granularity of function calls by using Intel MPK at user-level to isolate components. At the same time, it supports zero-copy data access across cubicles with feature-rich OS functionality. Our evaluation shows that CubicleOS introduces moderate end-to-end performance overheads in complex applications: 2\texttimes{} for the I/O-intensive NGINX web server with 8&nbsp;partitions, and 1.7–8\texttimes{} for the SQLite database engine with 7&nbsp;partitions.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {546–558},
numpages = {13},
keywords = {Intel MPK, compartments, inter-process communication, isolation},
location = {Virtual, USA},
series = {ASPLOS '21}
}

@inproceedings{eurosys22-mehrab,
author = {Mehrab, A K M Fazla and Nikolaev, Ruslan and Ravindran, Binoy},
title = {Kite: lightweight critical service domains},
year = {2022},
isbn = {9781450391627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3492321.3519586},
doi = {10.1145/3492321.3519586},
abstract = {Converged multi-level secure (MLS) systems, such as Qubes OS or SecureView, heavily rely on virtualization and service virtual machines (VMs). Traditionally, driver domains - isolated VMs that run device drivers - and daemon VMs use full-blown general-purpose OSs. It seems that specialized lightweight OSs, known as unikernels, would be a better fit for those. Surprisingly, to this day, driver domains can only be built from Linux. We discuss how unikernels can be beneficial in this context - they improve security and isolation, reduce memory overheads, and simplify software configuration and deployment. We specifically propose to use unikernels that borrow device drivers from existing general-purpose OSs.We present Kite which implements network and storage unikernel-based VMs and serve two essential classes of devices. We compare our approach against Linux using a number of typical micro- and macrobenchmarks used for networking and storage. Our approach achieves performance similar to that of Linux. However, we demonstrate that the number of system calls and ROP gadgets can be greatly reduced with our approach compared to Linux. We also demonstrate that our approach has resilience to an array of CVEs (e.g., CVE-2021-35039, CVE-2016-4963, and CVE-2013-2072), smaller image size, and improved startup time. Finally, unikernelizing is doable for the remaining (non-driver) service VMs as evidenced by our unikernelized DHCP server.},
booktitle = {Proceedings of the Seventeenth European Conference on Computer Systems},
pages = {384–401},
numpages = {18},
keywords = {virtual machine, unikernel, hypervisor, Xen},
location = {Rennes, France},
series = {EuroSys '22}
}

@inproceedings{asplos22-lefeuvre,
author = {Lefeuvre, Hugo and B\u{a}doiu, Vlad-Andrei and Jung, Alexander and Teodorescu, Stefan Lucian and Rauch, Sebastian and Huici, Felipe and Raiciu, Costin and Olivier, Pierre},
title = {FlexOS: towards flexible OS isolation},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507759},
doi = {10.1145/3503222.3507759},
abstract = {At design time, modern operating systems are locked in a specific safety and isolation strategy that mixes one or more hardware/software protection mechanisms (e.g. user/kernel separation); revisiting these choices after deployment requires a major refactoring effort. This rigid approach shows its limits given the wide variety of modern applications' safety/performance requirements, when new hardware isolation mechanisms are rolled out, or when existing ones break.  We present FlexOS, a novel OS allowing users to easily specialize the safety and isolation strategy of an OS at compilation/deployment time instead of design time. This modular LibOS is composed of fine-grained components that can be isolated via a range of hardware protection mechanisms with various data sharing strategies and additional software hardening. The OS ships with an exploration technique helping the user navigate the vast safety/performance design space it unlocks. We implement a prototype of the system and demonstrate, for several applications (Redis/Nginx/SQLite), FlexOS' vast configuration space as well as the efficiency of the exploration technique: we evaluate 80 FlexOS configurations for Redis and show how that space can be probabilistically subset to the 5 safest ones under a given performance budget. We also show that, under equivalent configurations, FlexOS performs similarly or better than existing solutions which use fixed safety configurations.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {467–482},
numpages = {16},
keywords = {Isolation, Operating Systems, Security},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@inproceedings {osdi23-chen,
author = {Jiahao Chen and Dingji Li and Zeyu Mi and Yuxuan Liu and Binyu Zang and Haibing Guan and Haibo Chen},
title = {Security and Performance in the Delegated User-level Virtualization},
booktitle = {17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
year = {2023},
isbn = {978-1-939133-34-2},
address = {Boston, MA},
pages = {209--226},
url = {https://www.usenix.org/conference/osdi23/presentation/chen},
publisher = {USENIX Association},
month = jul
}

@inproceedings{sosp23-wu,
author = {Wu, Fangnuo and Dong, Mingkai and Mo, Gequan and Chen, Haibo},
title = {TreeSLS: A Whole-system Persistent Microkernel with Tree-structured State Checkpoint on NVM},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613160},
doi = {10.1145/3600006.3613160},
abstract = {Whole-system persistence promises simplified application deployment and near-instantaneous recovery. This can be implemented using single-level store (SLS) through periodic checkpointing of ephemeral state to persistent devices. However, traditional SLSs suffer from two main issues on checkpointing efficiency and external synchrony, which are critical for low-latency services with persistence need.In this paper, we note that the decentralized state of microkernel-based systems can be exploited to simplify and optimize state checkpointing. To this end, we propose TreeSLS, a whole-system persistent microkernel that simplifies the whole-system state maintenance to a capability tree and a failure-resilient checkpoint manager. TreeSLS further exploits the emerging non-volatile memory to minimize checkpointing pause time by eliminating the distinction between ephemeral and persistent devices. With efficient state maintenance, TreeSLS further proposes delayed external visibility to provide transparent external synchrony with little overhead. Evaluation on microbenchmarks and real-world applications (e.g., Memcached, Redis and RocksDB) show that TreeSLS can complete a whole-system persistence in around 100 μs and even take a checkpoint every 1 ms with reasonable overhead to applications.},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {1–16},
numpages = {16},
keywords = {single-level store, microkernel, non-volatile memory, checkpoint/restore, transparent persistence},
location = {Koblenz, Germany},
series = {SOSP '23}
}

@inproceedings{eurosys23-lupu,
author = {Lupu, Costin and Albiundefinedoru, Andrei and Nichita, Radu and Bl\^{a}nzeanu, Doru-Florin and Pogonaru, Mihai and Deaconescu, R\u{a}zvan and Raiciu, Costin},
title = {Nephele: Extending Virtualization Environments for Cloning Unikernel-based VMs},
year = {2023},
isbn = {9781450394871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3552326.3587454},
doi = {10.1145/3552326.3587454},
abstract = {Unikernels gained an increasing interest in the recent years because they provide efficient resource allocation and high performance for cloud services by bundling the application with a minimal set of OS services in a guest VM. Although a unikernel is by design small and lightweight, fleets of unikernels based on the same image are not necessarily more efficient than containers because the latter can rely upon OS primitives for sharing memory. Furthermore, porting POSIX applications on top of unikernels brings a new challenge: what does fork() mean in the world of unikernels where there is memory isolation within a VM? Lacking fork() support significantly reduces the applicability of unikernels in popular cloud applications.In this paper we address these shortcomings and show that cloning unikernels makes way to further improvements and enables full functionality of popular cloud applications, such as NGINX and Redis. Our solution, Nephele, extends the Xen virtualization platform and provides autoscaling capabilities to unikernel based VMs. Nephele provides 8x faster instantiation times and can run 3x more active unikernel VMs on the same hardware compared to booting separate unikernels.},
booktitle = {Proceedings of the Eighteenth European Conference on Computer Systems},
pages = {574–589},
numpages = {16},
keywords = {virtualization, unikernels, Xen},
location = {Rome, Italy},
series = {EuroSys '23}
}

@inproceedings{eurosys23-raza,
author = {Raza, Ali and Unger, Thomas and Boyd, Matthew and Munson, Eric B and Sohal, Parul and Drepper, Ulrich and Jones, Richard and De Oliveira, Daniel Bristot and Woodman, Larry and Mancuso, Renato and Appavoo, Jonathan and Krieger, Orran},
title = {Unikernel Linux (UKL)},
year = {2023},
isbn = {9781450394871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3552326.3587458},
doi = {10.1145/3552326.3587458},
abstract = {This paper presents Unikernel Linux (UKL), a path toward integrating unikernel optimization techniques in Linux, a general purpose operating system. UKL adds a configuration option to Linux allowing for a single, optimized process to link with the kernel directly, and run at supervisor privilege. This UKL process does not require application source code modification, only a re-link with our, slightly modified, Linux kernel and glibc. Unmodified applications show modest performance gains out of the box, and developers can further optimize applications for more significant gains (e.g. 26\% throughput improvement for Redis). UKL retains support for co-running multiple user level processes capable of communicating with the UKL process using standard IPC. UKL preserves Linux's battle-tested codebase, community, and ecosystem of tools, applications, and hardware support. UKL runs both on bare-metal and virtual servers and supports multi-core execution. The changes to the Linux kernel are modest (1250 LOC).},
booktitle = {Proceedings of the Eighteenth European Conference on Computer Systems},
pages = {590–605},
numpages = {16},
keywords = {unikernels, linux, specialized operating systems},
location = {Rome, Italy},
series = {EuroSys '23}
}

@inproceedings {atc23-heinloth,
author = {Bernhard Heinloth and Peter W{\"a}gemann and Wolfgang Schr{\"o}der-Preikschat},
title = {Luci: Loader-based Dynamic Software Updates for Off-the-shelf Shared Objects},
booktitle = {2023 USENIX Annual Technical Conference (USENIX ATC 23)},
year = {2023},
isbn = {978-1-939133-35-9},
address = {Boston, MA},
pages = {241--256},
url = {https://www.usenix.org/conference/atc23/presentation/heinloth},
publisher = {USENIX Association},
month = jul
}

@inproceedings {atc23-yasukata,
author = {Kenichi Yasukata and Hajime Tazaki and Pierre-Louis Aublin and Kenta Ishiguro},
title = {zpoline: a system call hook mechanism based on binary rewriting},
booktitle = {2023 USENIX Annual Technical Conference (USENIX ATC 23)},
year = {2023},
isbn = {978-1-939133-35-9},
address = {Boston, MA},
pages = {293--300},
url = {https://www.usenix.org/conference/atc23/presentation/yasukata},
publisher = {USENIX Association},
month = jul
}

@inproceedings {atc23-wrenger,
author = {Lars Wrenger and Florian Rommel and Alexander Halbuer and Christian Dietrich and Daniel Lohmann},
title = {{LLFree}: Scalable and {Optionally-Persistent} {Page-Frame} Allocation},
booktitle = {2023 USENIX Annual Technical Conference (USENIX ATC 23)},
year = {2023},
isbn = {978-1-939133-35-9},
address = {Boston, MA},
pages = {897--914},
url = {https://www.usenix.org/conference/atc23/presentation/wrenger},
publisher = {USENIX Association},
month = jul
}

@inproceedings{asplos23-ma,
author = {Ma, Teng and Chen, Shanpei and Wu, Yihao and Deng, Erwei and Song, Zhuo and Chen, Quan and Guo, Minyi},
title = {Efficient Scheduler Live Update for Linux Kernel with Modularization},
year = {2023},
isbn = {9781450399180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582016.3582054},
doi = {10.1145/3582016.3582054},
abstract = {The scheduler is a critical component of the operating system (OS)and is tightly coupled with Linux. Production-level clouds often host various workloads, and these workloads require different schedulers to achieve high performance. Thus the capability of updating the scheduler lively without rebooting the OS is crucial for the production environments. However, emerging live update techniques only apply for the fine-grained function-level updates or require extra constraints such as microkernel. It fails to update the entire heavy process scheduler subsystem lively.
We therefore propose Plugsched to enable scheduler live update, and there are two key novelties. First of all, with the idea of modularization, Plugsched decouples the scheduler from the Linux kernel to be an independent module; Secondly, Plugsched uses the data rebuild technique to migrate the state from the old scheduler to the new one. This scheme can be directly applied to the Linux kernel scheduler in production environments without modifying kernel code. Unlike current function-level live update solutions, Plugsched allows developers to update the entire scheduler subsystem and modify internal scheduler data via the rebuilding technique. Moreover, an optimized stack inspection method is introduced to further effectively reduce the downtime due to the update. Experimental and production results show that Plugsched can effectively update kernel scheduler lively and the downtime is less than tens of milliseconds.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {194–207},
numpages = {14},
keywords = {Linux scheduler, kernel, live update},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings {298677,
author = {Haoran Ma and Yifan Qiao and Shi Liu and Shan Yu and Yuanjiang Ni and Qingda Lu and Jiesheng Wu and Yiying Zhang and Miryung Kim and Harry Xu},
title = {{DRust}: {Language-Guided} Distributed Shared Memory with Fine Granularity, Full Transparency, and Ultra Efficiency},
booktitle = {18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
year = {2024},
isbn = {978-1-939133-40-3},
address = {Santa Clara, CA},
pages = {97--115},
url = {https://www.usenix.org/conference/osdi24/presentation/ma-haoran},
publisher = {USENIX Association},
month = jul,
code = {https://github.com/uclasystem/DRust},
}

@inproceedings {298707,
author = {Hayley LeBlanc and Nathan Taylor and James Bornholt and Vijay Chidambaram},
title = {{SquirrelFS}: using the Rust compiler to check file-system crash consistency},
booktitle = {18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
year = {2024},
isbn = {978-1-939133-40-3},
address = {Santa Clara, CA},
pages = {387--404},
url = {https://www.usenix.org/conference/osdi24/presentation/leblanc},
publisher = {USENIX Association},
month = jul,
code = {https://github.com/utsaslab/squirrelfs},
}

@inproceedings {285736,
author = {Jian Gao and Youyou Lu and Minhui Xie and Qing Wang and Jiwu Shu},
title = {Citron: Distributed Range Lock Management with One-sided {RDMA}},
booktitle = {21st USENIX Conference on File and Storage Technologies (FAST 23)},
year = {2023},
isbn = {978-1-939133-32-8},
address = {Santa Clara, CA},
pages = {297--314},
url = {https://www.usenix.org/conference/fast23/presentation/gao},
publisher = {USENIX Association},
month = feb
}

@inproceedings {305236,
author = {Jian Gao and Qing Wang and Jiwu Shu},
title = {{ShiftLock}: Mitigate One-sided {RDMA} Lock Contention via Handover},
booktitle = {23rd USENIX Conference on File and Storage Technologies (FAST 25)},
year = {2025},
isbn = {978-1-939133-45-8},
address = {Santa Clara, CA},
pages = {355--372},
url = {https://www.usenix.org/conference/fast25/presentation/gao},
publisher = {USENIX Association},
month = feb,
code = {https://github.com/thustorage/shiftlock},
}

@inproceedings{sosp24-vuppalapati,
author = {Vuppalapati, Midhul and Agarwal, Rachit},
title = {Tiered Memory Management: Access Latency is the Key!},
year = {2024},
isbn = {9798400712517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3694715.3695968},
doi = {10.1145/3694715.3695968},
abstract = {The emergence of tiered memory architectures has led to a renewed interest in memory management. Recent works on tiered memory management innovate on mechanisms for access tracking, page migration, and dynamic page size determination; however, they all use the same page placement algorithm---packing the hottest pages in the default tier (one with the lowest hardware-specified memory access latency). This makes an implicit assumption that, despite serving the hottest pages, the access latency of the default tier is less than that of alternate tiers. This assumption is far from real: it is well-known in the computer architecture community that, in the realistic case of multiple in-flight requests, memory access latency can be significantly larger than the hardware-specified latency. We show that, even under moderate loads, the default tier access latency can inflate to be 2.5\texttimes{} larger than the latency of alternate tiers; and that, under this regime, performance of state-of-the-art memory tiering systems can be 2.3\texttimes{} worse than the optimal.Colloid is a memory management mechanism that embodies the principle of balancing access latencies---page placement across tiers should be performed so as to balance their average (loaded) access latencies. To realize this principle, Colloid innovates on both per-tier memory access latency measurement mechanisms, and page placement algorithms that decide the set of pages to place in each tier. We integrate Colloid with three state-of-the-art memory tiering systems---HeMem, TPP and MEMTIS. Evaluation across a wide variety of workloads demonstrates that Colloid consistently enables the underlying system to achieve near-optimal performance.},
booktitle = {Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles},
pages = {79–94},
numpages = {16},
keywords = {operating systems, tiered memory management},
location = {Austin, TX, USA},
series = {SOSP '24}
}

@inproceedings{sosp24-dwivedi,
author = {Dwivedi, Kumar Kartikeya and Iyer, Rishabh and Kashyap, Sanidhya},
title = {Fast, Flexible, and Practical Kernel Extensions},
year = {2024},
isbn = {9798400712517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3694715.3695950},
doi = {10.1145/3694715.3695950},
abstract = {The ability to safely extend OS kernel functionality is a longstanding goal in OS design, with the widespread use of the eBPF framework in Linux and Windows demonstrating the benefits of such extensibility. However, existing solutions for kernel extensibility (including eBPF) are limited and constrain users either in the extent of functionality that they can offload to the kernel or the performance overheads incurred by their extensions.We present KFlex: a new approach to kernel extensibility that strikes an improved balance between the expressivity and performance of kernel extensions. To do so, KFlex separates the safety of kernel-owned resources (e.g., kernel memory) from the safety of extension-specific resources (e.g., extension memory). This separation enables KFlex to use distinct, bespoke mechanisms to enforce each safety property---automated verification and lightweight runtime checks, respectively---which enables the offload of diverse functionality while incurring low runtime overheads.We realize KFlex in the context of Linux. We demonstrate that KFlex enables users to offload functionality that cannot be offloaded today and provides significant end-to-end performance benefits for applications. Several of KFlex's proposed mechanisms have been upstreamed into the Linux kernel mainline, with efforts ongoing for full integration.},
booktitle = {Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles},
pages = {249–264},
numpages = {16},
location = {Austin, TX, USA},
series = {SOSP '24}
}

@inproceedings{sosp24-jia,
author = {Jia, Yuekai and Tian, Kaifu and You, Yuyang and Chen, Yu and Chen, Kang},
title = {Skyloft: A General High-Efficient Scheduling Framework in User Space},
year = {2024},
isbn = {9798400712517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3694715.3695973},
doi = {10.1145/3694715.3695973},
abstract = {Skyloft is a general and highly efficient user-space scheduling framework. It leverages user-mode interrupt to deliver and process hardware timers directly in user space. This capability enables Skyloft to achieve μs-scale preemption. Skyloft offers a set of scheduling interfaces that supports different scheduling policies, including both preemptive and nonpreemptive ones. Operating as a user-space scheduling framework, Skyloft is compatible with Linux and integrates seamlessly with high-performance I/O frameworks like DPDK.Evaluation results show that optimizations in per-CPU scheduling with user-space timer interrupts allow Skyloft's Completely Fair Scheduler (CFS) and Round Robin (RR) to significantly reduce wake-up latency compared to their Linux counterparts (100μs vs. 10000μs). In comparison to the general scheduling framework ghOSt, Skyloft achieves a 1.2\texttimes{} increase in maximum throughput for Latency Critical (LC) applications. Additionally, unlike the specialized scheduling framework Shinjuku, Skyloft not only supports LC applications but also efficiently allocates CPU resources to Best Effort (BE) applications under low load conditions. By incorporating a 5μs preemption mechanism into the Work-Stealing strategy, a RocksDB Server running on Skyloft exhibits a performance improvement of 1.9\texttimes{} compared to Shenango.},
booktitle = {Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles},
pages = {265–279},
numpages = {15},
keywords = {operating systems, scheduling, user interrupts},
location = {Austin, TX, USA},
series = {SOSP '24}
}

@inproceedings{sosp24-lin,
author = {Lin, Jiazhen and Chen, Youmin and Gao, Shiwei and Lu, Youyou},
title = {Fast Core Scheduling with Userspace Process Abstraction},
year = {2024},
isbn = {9798400712517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3694715.3695976},
doi = {10.1145/3694715.3695976},
abstract = {We introduce uProcess, a pure userspace process abstraction that enables CPU cores to be rescheduled among applications at sub-microsecond timescale without trapping into the kernel. We achieve this by constructing a special privileged mode in userspace to achieve safe and efficient separation among uProcesses. The core idea is a careful combination of two emerging hardware features - userspace interrupts (Uintr) and memory protection keys (MPK). We materialize the uProcess abstraction by implementing Vessel, a userspace core scheduler that colocates latency-critical and best-effort applications with minimal switching overhead when they time-share CPU cores. Our experiment result shows that Vessel exhibits better overall performance and low latency when multiple applications are colocated.},
booktitle = {Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles},
pages = {280–295},
numpages = {16},
location = {Austin, TX, USA},
series = {SOSP '24}
}

@inproceedings{sosp24-luo,
author = {Luo, Xuhao and Bhat, Shreesha G. and Hu, Jiyu and Alagappan, Ramnatthan and Ganesan, Aishwarya},
title = {LazyLog: A New Shared Log Abstraction for Low-Latency Applications},
year = {2024},
isbn = {9798400712517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3694715.3695983},
doi = {10.1145/3694715.3695983},
abstract = {Shared logs offer linearizable total order across storage shards. However, they enforce this order eagerly upon ingestion, leading to high latencies. We observe that in many modern shared-log applications, while linearizable ordering is necessary, it is not required eagerly when ingesting data but only later when data is consumed. Further, readers are naturally decoupled in time from writers in these applications. Based on this insight, we propose LazyLog, a novel shared log abstraction. LazyLog lazily binds records (across shards) to linearizable global positions and enforces this before a log position can be read. Such lazy ordering enables low ingestion latencies. Given the time decoupling, LazyLog can establish the order well before reads arrive, minimizing overhead upon reads. We build two LazyLog systems that provide linearizable total order across shards. Our experiments show that LazyLog systems deliver significantly lower latencies than conventional, eager-ordering shared logs.},
booktitle = {Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles},
pages = {296–312},
numpages = {17},
location = {Austin, TX, USA},
series = {SOSP '24}
}

@inproceedings{sosp24-guo,
author = {Guo, Kaijie and Li, Dingji and Luo, Ben and Shen, Yibin and Peng, Kaihuan and Luo, Ning and Dai, Shengdong and Liang, Chen and Song, Jianming and Yang, Hang and Zhang, Xiantao and Mi, Zeyu},
title = {VPRI: Efficient I/O Page Fault Handling via Software-Hardware Co-Design for IaaS Clouds},
year = {2024},
isbn = {9798400712517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3694715.3695957},
doi = {10.1145/3694715.3695957},
abstract = {Device pass-through has been widely adopted by cloud service providers to achieve near bare-metal I/O performance in virtual machines (VMs). However, this approach requires static pinning of VM memory, making on-demand paging unavailable. The hardware device I/O page fault (IOPF) capability offers an optimal solution to this limitation. Current IOPF approaches, using either standard IOMMU capabilities (ATS+PRI) or devices with independent IOMMU implementations, have not gained widespread adoption in public Infrastructure-as-a-Service clouds. This is due to high costs, platform dependency, and significant impacts on performance and service level objectives (SLOs). We present the Virtualized Page Request Interface (VPRI), a novel IOPF system developed through software-hardware collaboration. VPRI is not only platform-independent, free from address translation complexities, but also cost-effective, and designed to minimize SLO impact. Our work enables large-scale deployment of IOPF capability in Alibaba Cloud with negligible impact on SLOs. When integrated with memory management software, it significantly enhances memory utilization in public IaaS clouds, effectively overcoming the static memory pinning restriction associated with pass-through devices.},
booktitle = {Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles},
pages = {541–557},
numpages = {17},
keywords = {page fault, virtualization, cloud computing, service level objective},
location = {Austin, TX, USA},
series = {SOSP '24}
}

@inproceedings{sosp24-qiu,
author = {Qiu, Jiaxing and Zhou, Zijie and Li, Yang and Li, Zhenhua and Qian, Feng and Lin, Hao and Gao, Di and Su, Haitao and Miao, Xin and Liu, Yunhao and Xu, Tianyin},
title = {vSoC: Efficient Virtual System-on-Chip on Heterogeneous Hardware},
year = {2024},
isbn = {9798400712517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3694715.3695946},
doi = {10.1145/3694715.3695946},
abstract = {Emerging mobile apps such as UHD video and AR/VR access diverse high-throughput hardware devices, e.g., video codecs, cameras, and image processors. However, today's mobile emulators exhibit poor performance when emulating these devices. We pinpoint the major reason to be the discrepancy between the guest's and host's memory architectures for hardware devices, i.e., the mobile guest's centralized memory on a system-on-chip (SoC) versus the PC/server's separated memory modules on individual hardware. Such a discrepancy makes the shared virtual memory (SVM) architecture of mobile emulators highly inefficient.To address this, we design and implement vSoC, the first virtual mobile SoC that enables virtual devices to efficiently share data through a unified SVM framework. We then build upon the SVM framework a prefetch engine that effectively hides the overhead of coherence maintenance (which guarantees that devices sharing the same virtual memory see the same data), a performance bottleneck in existing emulators. Compared to state-of-the-art emulators, vSoC brings 12\%-49\% higher frame rates to top popular mobile apps, while achieving 1.8--9.0\texttimes{} frame rates and 35\%-62\% lower motion-to-photon latency for emerging apps. vSoC is adopted by Huawei DevEco Studio, a major mobile IDE.},
booktitle = {Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles},
pages = {558–573},
numpages = {16},
keywords = {virtualization, mobile systems, system-on-chip, shared memory},
location = {Austin, TX, USA},
series = {SOSP '24}
}

@inproceedings{eurosys24-miller,
author = {Miller, Samantha and Kumar, Anirudh and Vakharia, Tanay and Chen, Ang and Zhuo, Danyang and Anderson, Thomas},
title = {Enoki: High Velocity Linux Kernel Scheduler Development},
year = {2024},
isbn = {9798400704376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627703.3629569},
doi = {10.1145/3627703.3629569},
abstract = {Kernel task scheduling is important for application performance, adaptability to new hardware, and complex user requirements. However, developing, testing, and debugging new scheduling algorithms in Linux, the most widely used cloud operating system, is slow and difficult. We developed Enoki, a framework for high velocity development of Linux kernel schedulers. Enoki schedulers are written in safe Rust, and the system supports live upgrade of new scheduling policies into the kernel, userspace debugging, and bidirectional communication with applications. A scheduler implemented with Enoki achieved near identical performance (within 1\% on average) to the default Linux scheduler CFS on a wide range of benchmarks. Enoki is also able to support a range of research schedulers, specifically the Shinjuku scheduler, a locality aware scheduler, and the Arachne core arbiter, with good performance.},
booktitle = {Proceedings of the Nineteenth European Conference on Computer Systems},
pages = {962–980},
numpages = {19},
keywords = {development velocity, kernel development, scheduling},
location = {Athens, Greece},
series = {EuroSys '24}
}

@inproceedings {atc24-li,
author = {Hongyu Li and Liwei Guo and Yexuan Yang and Shangguang Wang and Mengwei Xu},
title = {An Empirical Study of {Rust-for-Linux}: The Success, Dissatisfaction, and Compromise},
booktitle = {2024 USENIX Annual Technical Conference (USENIX ATC 24)},
year = {2024},
isbn = {978-1-939133-41-0},
address = {Santa Clara, CA},
pages = {425--443},
url = {https://www.usenix.org/conference/atc24/presentation/li-hongyu},
publisher = {USENIX Association},
month = jul
}

@inproceedings {atc24-gao,
author = {Bin Gao and Qingxuan Kang and Hao-Wei Tee and Kyle Timothy Ng Chu and Alireza Sanaee and Djordje Jevdjic},
title = {Scalable and Effective Page-table and {TLB} management on {NUMA} Systems},
booktitle = {2024 USENIX Annual Technical Conference (USENIX ATC 24)},
year = {2024},
isbn = {978-1-939133-41-0},
address = {Santa Clara, CA},
pages = {445--461},
url = {https://www.usenix.org/conference/atc24/presentation/gao-bin-scalable},
publisher = {USENIX Association},
month = jul
}

@inproceedings{asplos24-qu,
author = {Qu, Hongliang and Yu, Zhibin},
title = {WASP: Workload-Aware Self-Replicating Page-Tables for NUMA Servers},
year = {2024},
isbn = {9798400703850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620665.3640369},
doi = {10.1145/3620665.3640369},
abstract = {Recently, page-table self-replication (PTSR) has been proposed to reduce the page-table caused NUMA effect for large-memory workloads on NUMA servers. However, PTSR may improve or hurt performance of an application, depending on its characteristics and the co-located applications. This is hard for users to know, but current PTSR can only be manually enabled/disabled by users.To address this issue, we propose WASP (Workload-Aware Self-Replication) to automatically enable/disable PTSR to reduce the page-table caused NUMA effect. WASP innovates two techniques. First, it identifies a set of indicators, which are generally available on most processor architectures, to indicate if PTSR should be enabled/disabled. Second, WASP devises a hierarchical as well as gradual mechanism using these indicators to enable/disable PTSR automatically for a specific workload to reduce the page-table caused NUMA effect during its execution.We implement WASP in Linux and evaluate it on x86 and ARM NUMA servers. Experimental results show that WASP can automatically enable/disable PTSR successfully according to workload characteristics, achieving at least the same performance improvement as that obtained by manually enabling PTSR on both x86 and ARM NUMA servers.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {1233–1249},
numpages = {17},
location = {La Jolla, CA, USA},
series = {ASPLOS '24}
}
