# Literature on Model Inference Systems

A reading list for model inference systems, including but not limited to related research on software and hardware level. The list covers related papers, articles, tools, blogs, courses and other resources.

## Academic Papers

<!--
****************************************************************************
*                                                                          *
*    MAINTANANCE NOTE:                                                     *
*    - Please ensure that all papers are linked correctly and that the     *
*      information is up to date.                                          *
*    - Please order the papers by:                                         *
*      1. publication date.                                                *
*      2. conference / journal name.                                       *
*      3. alphabetical order of the paper title.                           *
*                                                                          *
****************************************************************************
-->

- **[ASPLOS 25]** MoE-Lightning: High-Throughput MoE Inference on Memory-constrained GPUs
  - Authors: Shiyi Cao, Shu Liu, Tyler Griggs, Peter Schafhalter, Xiaoxuan Liu, Ying Sheng, Joseph E. Gonzalez, Matei Zaharia, and Ion Stoica
  - [Paper](https://doi.org/10.1145/3669940.3707267), [Code](https://github.com/caoshiyi/artifacts/blob/asplos25)
- **[FAST 25]** Mooncake: Trading More Storage for Less Computation â€” A KVCache-centric Architecture for Serving LLM Chatbot
  - Authors: Ruoyu Qin and Zheming Li and Weiran He and Jialei Cui and Feng Ren and Mingxing Zhang and Yongwei Wu and Weimin Zheng and Xinran Xu
  - [Paper](https://www.usenix.org/conference/fast25/presentation/qin), [Code](https://github.com/kvcache-ai/Mooncake)
- **[OSDI 24]** DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving
  - Authors: Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang
  - [Paper](https://www.usenix.org/conference/osdi24/presentation/zhong-yinmin), [Code](https://github.com/LLMServe/DistServe)
- **[OSDI 24]** Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve
  - Authors: Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gulavani, Alexey Tumanov, and Ramachandran Ramjee
  - [Paper](https://www.usenix.org/conference/osdi24/presentation/agrawal), [Code](https://github.com/microsoft/sarathi-serve)
- **[SOSP 24]** PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU
  - Authors: Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen
  - [Paper](https://doi.org/10.1145/3694715.3695964), [Code](https://github.com/SJTU-IPADS/PowerInfer)
- **[SOSP 23]** Efficient Memory Management for Large Language Model Serving with PagedAttention
  - Authors: Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica
  - [Paper](https://doi.org/10.1145/3600006.3613165), [Code](https://github.com/vllm-project/vllm)
- **[OSDI 22]** Orca: A Distributed Serving System for Transformer-Based Generative Models
  - Authors: Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and  Byung-Gon Chun
  - [Paper](https://www.usenix.org/conference/osdi22/presentation/yu)
